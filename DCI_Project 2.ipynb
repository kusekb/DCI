{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Popularity of Posts on Hacker News\n",
    "\n",
    "In this project, we want to know  what types of posts on the [Hacker News website](https://news.ycombinator.com/) are the most popular. Specifically, we're only interested in posts that start with \"Ask HN\" and \"Show HN\" since these posts are of a specific type: *asking* and *showing* type. We want to know which of the two types are more popular by comparison of total comment numbers. We'll also want to find what time periods are the most active for commenting on the website in general. \n",
    "\n",
    "We'll use a [Kaggle dataset](https://www.kaggle.com/hacker-news/hacker-news-posts) with around 300,000 rows (posts) of data for this analysis. In the [discussion section](https://www.kaggle.com/hacker-news/hacker-news-posts/discussion) of the dataset, we can see no error discussions despite the dataset being out for 3 years now with 20,000 views and 3.5k downloads. So, it's safe to assume the dataset is clean and free of problems.\n",
    "\n",
    "## 1. Opening the Dataset\n",
    "\n",
    "First, we need to open the dataset. We'll do this the usual way by importing the *reader()* function from the *csv* module and open our dataset. Then, we'll use Python's built-in *list()* function to turn our file object into a list of lists that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "opened_file = open(\"hacker_news.csv\")\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's print the first 5 rows of data to see what the rows generally look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'] \n",
      "\n",
      "['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'] \n",
      "\n",
      "['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'] \n",
      "\n",
      "['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5): # A for loop that runs 5 times, printing a new line after each iteration.\n",
    "    print(hn[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Removing the Header Row\n",
    "\n",
    "Before we act on this dataset, it's custom to remove the header row to make our code more readable. We'll assign the header row to a variable named *headers*, then print it so we can view our column data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "headers = hn[0] # Index 0 is the header row.\n",
    "hn = hn[1:]\n",
    "\n",
    "print(headers,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are relatively straightforward and we can already see what we're interested in. We want to know the *num_comments* for each post, and when it was *created_at*. But first, before we look at the data in those columns, let's make sure that our header row is removed from the dataset by printing the first 5 rows (posts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'] \n",
      "\n",
      "['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'] \n",
      "\n",
      "['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'] \n",
      "\n",
      "['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'] \n",
      "\n",
      "['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(hn[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Narrowing Posts Down to \"Ask HN\" and \"Show HN\"\n",
    "\n",
    "Since the thousands of rows of data in our dataset can be daunting to analyze, we'll narrow our focus to \"Ask HN\" and \"Show HN\" posts, the reason being that these posts have a clear genre type. Any other types of post will simply be categorized as \"Other\".\n",
    "\n",
    "Let's isolate the *ask* and *show* types of posts by searching for any posts that start with those strings, *Ask HN* and *Show HN*. But we'll also need to do some **case** checking, making sure that any grammatical casing of \"ask hn\" or \"SHOW HN\" (for example) makes it through our data cleansing. Since this would be a tedious task, a simpler way is to just convert the title into lowercase and ignore casing. \n",
    "\n",
    "We'll sort out these posts with a switch statement that will append each row from the original dataset into one of three new list-of-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 3 empty lists for each type of post, ask type, show type, or other.\n",
    "\n",
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1] # Title of post is at index 1.\n",
    "\n",
    "    if title.lower().startswith(\"ask hn\"):    # Ignore the check for casing by removing the casing entirely.\n",
    "        ask_posts.append(row)                 # If we have a matching title, then append the post data to the new list.\n",
    "    elif title.lower().startswith(\"show hn\"): # We must use elif here, otherwise we can have duplicate rows.\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)               # All other types of posts are discarded here in this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that our sorting method worked, let's first do some simple arithmetic to check if the total of *ask_posts* + *show_posts* + *other_posts* == *hn*. Then, let's print the first few rows of each dataset to make sure that the posts in *ask_posts* start with \"ask hn\", the posts in *show_posts* start with \"show hn\", and that the posts in *other_posts* don't contain either \"ask hn\" or \"show hn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9139\n",
      "10158\n",
      "273822 \n",
      "\n",
      "293119\n",
      "293119\n"
     ]
    }
   ],
   "source": [
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts), \"\\n\")\n",
    "\n",
    "print(len(ask_posts) + len(show_posts) + len(other_posts)) \n",
    "print(len(hn)) # The bottom two outputs should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12578908', 'Ask HN: What TLD do you use for local development?', '', '4', '7', 'Sevrene', '9/26/2016 2:53'] \n",
      "\n",
      "['12578522', 'Ask HN: How do you pass on your work when you die?', '', '6', '3', 'PascLeRasc', '9/26/2016 1:17'] \n",
      "\n",
      "['12577908', 'Ask HN: How a DNS problem can be limited to a geographic region?', '', '1', '0', 'kuon', '9/25/2016 22:57'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print(ask_posts[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12578335', 'Show HN: Finding puns computationally', 'http://puns.samueltaylor.org/', '2', '0', 'saamm', '9/26/2016 0:36'] \n",
      "\n",
      "['12578182', 'Show HN: A simple library for complicated animations', 'https://christinecha.github.io/choreographer-js/', '1', '0', 'christinecha', '9/26/2016 0:01'] \n",
      "\n",
      "['12578098', 'Show HN: WebGL visualization of DNA sequences', 'http://grondilu.github.io/dna.html', '1', '0', 'grondilu', '9/25/2016 23:44'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print(show_posts[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'] \n",
      "\n",
      "['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'] \n",
      "\n",
      "['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print(other_posts[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our sorting method worked, and that our output for the length of our sum is equal to *hn* as expected. Now that we've created our three new list-of-lists, we can find the average number of comments for each type of post: \"Ask HN\" and \"Show HN\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Averaging the Number of Comments for \"Ask HN\" and \"Show HN\" Posts\n",
    "\n",
    "We'll easily calculate the average by looping through the *ask_posts* and *show_posts* list-of-lists, summing up the number of comments, and then dividing by the length of the list-of-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.393478498741656\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    num_comments = int(row[4]) # num_comments is at index 4 of the dataset.\n",
    "    total_ask_comments += num_comments\n",
    "    \n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "\n",
    "print(avg_ask_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.886099625910612\n"
     ]
    }
   ],
   "source": [
    "total_show_comments = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    num_comments = int(row[4])\n",
    "    total_show_comments += num_comments\n",
    "    \n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "\n",
    "print(avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the outputs, we can see that *ask* type posts receive much more comments on average than *show* posts. This succinctly answers our first question as to which type of post was more popular on the website, but what time is the best to post in general?\n",
    "\n",
    "## 5. Grouping \"Ask HN\" Posts by Hour Created\n",
    "\n",
    "Since we've decided that *ask* type posts are more popular, let's do the time analysis on the *ask_posts* list-of-lists. Earlier, we chose the *created_at* column of data to be our time reference, so let's create a new list-of-lists called *result_list* that will store lists with two values: the creation date of the post, and the number of comments it received. We'll print out a sample row to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9/26/2016 2:53', 7] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    num_comments = int(row[4])\n",
    "    result_list.append([created_at, num_comments])  \n",
    "\n",
    "print(result_list[0], \"\\n\")\n",
    "# \"8/16/2016 9:55\" is in the format mm/DD/YYYY HH:MM, or %m/%d/%Y %H:%M\n",
    "# in datetime format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to organize *result_list*. We now have a list-of-lists of the time each post was made, as well as how many comments it has received. But we want to know the average of how many comments an *ask* type posts receives per hour. To do this, we'll create two dictionaries, one to sum up the number of *ask* type comments per hour, and one to sum up the number of posts created. \n",
    "\n",
    "The keys in this dictionary will be the hour the post was created, and the values will be the number of comments or total number of posts. Before we calculate the average though, let's print out our dictionaries to make sure the sorting-by-hour worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'02': 269, '01': 282, '22': 383, '21': 518, '19': 552, '17': 587, '15': 646, '14': 513, '13': 444, '11': 312, '10': 282, '09': 222, '07': 226, '03': 271, '23': 343, '20': 510, '16': 579, '08': 257, '00': 301, '18': 614, '12': 342, '04': 243, '06': 234, '05': 209} \n",
      "\n",
      "{'02': 2996, '01': 2089, '22': 3372, '21': 4500, '19': 3954, '17': 5547, '15': 18525, '14': 4972, '13': 7245, '11': 2797, '10': 3013, '09': 1477, '07': 1585, '03': 2154, '23': 2297, '20': 4462, '16': 4466, '08': 2362, '00': 2277, '18': 4877, '12': 4234, '04': 2360, '06': 1587, '05': 1838}\n"
     ]
    }
   ],
   "source": [
    "counts_by_hour = {}\n",
    "comments_by_hour = {} \n",
    "\n",
    "for row in result_list:\n",
    "    created = row[0]\n",
    "    num = row[1]\n",
    "   \n",
    "    # Parse the time from the string and turn it into a datetime object.\n",
    "    time_stamp = dt.datetime.strptime(created, \"%m/%d/%Y %H:%M\")\n",
    "    \n",
    "    # Get the hour from the object.\n",
    "    time_at_hour = time_stamp.strftime(\"%H\")\n",
    "    \n",
    "    if time_at_hour in counts_by_hour:\n",
    "        comments_by_hour[time_at_hour] += num\n",
    "        counts_by_hour[time_at_hour] += 1\n",
    "    else:\n",
    "        comments_by_hour[time_at_hour] = num\n",
    "        counts_by_hour[time_at_hour] = 1\n",
    "\n",
    "print(counts_by_hour, \"\\n\")\n",
    "print(comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Averaging \"Ask HN\" Post Comments per Hour\n",
    "\n",
    "Our outputs are disorganized, but correct. There are 24 entries in each dictionary for each hour of the day.\n",
    "\n",
    "We have two dictionaries: both include the hours 0-23 as keys, but they differ in their values. *counts_by_hour* has values of the total number of posts per hour, while *comments_by_hour* has values of the total number of comments per hour.\n",
    "\n",
    "We want to know the average number of *ask* type comments per hour. To do this, we'll simply make a new list-of-lists and store each hour as a value next to the result of the total number of comments divided by the total number of posts for each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02', 11.137546468401487]\n",
      "['01', 7.407801418439717]\n",
      "['22', 8.804177545691905]\n",
      "['21', 8.687258687258687]\n",
      "['19', 7.163043478260869]\n",
      "['17', 9.449744463373083]\n",
      "['15', 28.676470588235293]\n",
      "['14', 9.692007797270955]\n",
      "['13', 16.31756756756757]\n",
      "['11', 8.96474358974359]\n",
      "['10', 10.684397163120567]\n",
      "['09', 6.653153153153153]\n",
      "['07', 7.013274336283186]\n",
      "['03', 7.948339483394834]\n",
      "['23', 6.696793002915452]\n",
      "['20', 8.749019607843136]\n",
      "['16', 7.713298791018998]\n",
      "['08', 9.190661478599221]\n",
      "['00', 7.5647840531561465]\n",
      "['18', 7.94299674267101]\n",
      "['12', 12.380116959064328]\n",
      "['04', 9.7119341563786]\n",
      "['06', 6.782051282051282]\n",
      "['05', 8.794258373205741]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for row in comments_by_hour:\n",
    "    comments = comments_by_hour[row]\n",
    "    counts = counts_by_hour[row]\n",
    "    avg_comments_and_count = comments / counts\n",
    "    avg_by_hour.append([row, avg_comments_and_count])\n",
    "\n",
    "counter = 0\n",
    "for i in avg_by_hour:\n",
    "    print(avg_by_hour[counter])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that again, we have 24 entries, our sorting method worked correctly. We now have the average number of *ask* comments per hour. However, it's really hard to understand the list we're currently looking at.\n",
    "\n",
    "## 7. Sorting the Averages\n",
    "\n",
    "Let's fix this problem by sorting this list and making it readable to non-programmers. First, we want to know which average is the highest to answer our initial question: which time is the best to post? Let's swap the columns of *avg_by_hour* so that that the average is the first value and see if the data is more readable. We'll make an empty, new, swapped list-of-lists to help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.137546468401487, '02']\n",
      "[7.407801418439717, '01']\n",
      "[8.804177545691905, '22']\n",
      "[8.687258687258687, '21']\n",
      "[7.163043478260869, '19']\n",
      "[9.449744463373083, '17']\n",
      "[28.676470588235293, '15']\n",
      "[9.692007797270955, '14']\n",
      "[16.31756756756757, '13']\n",
      "[8.96474358974359, '11']\n",
      "[10.684397163120567, '10']\n",
      "[6.653153153153153, '09']\n",
      "[7.013274336283186, '07']\n",
      "[7.948339483394834, '03']\n",
      "[6.696793002915452, '23']\n",
      "[8.749019607843136, '20']\n",
      "[7.713298791018998, '16']\n",
      "[9.190661478599221, '08']\n",
      "[7.5647840531561465, '00']\n",
      "[7.94299674267101, '18']\n",
      "[12.380116959064328, '12']\n",
      "[9.7119341563786, '04']\n",
      "[6.782051282051282, '06']\n",
      "[8.794258373205741, '05']\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    first = row[1] # Swap the columns.\n",
    "    second = row[0]\n",
    "    \n",
    "    swap_avg_by_hour.append([first, second]) # Append to a new list.\n",
    "    \n",
    "counter = 0\n",
    "for i in swap_avg_by_hour:\n",
    "    print(swap_avg_by_hour[counter])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still can't really tell what's going on, but we can fix this by sorting the averages using Python's built-in *sorted()* function. We'll set the variable *reverse* == *True* such that the averages are displayed in descending order, putting the highest average at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.676470588235293, '15']\n",
      "[16.31756756756757, '13']\n",
      "[12.380116959064328, '12']\n",
      "[11.137546468401487, '02']\n",
      "[10.684397163120567, '10']\n",
      "[9.7119341563786, '04']\n",
      "[9.692007797270955, '14']\n",
      "[9.449744463373083, '17']\n",
      "[9.190661478599221, '08']\n",
      "[8.96474358974359, '11']\n",
      "[8.804177545691905, '22']\n",
      "[8.794258373205741, '05']\n",
      "[8.749019607843136, '20']\n",
      "[8.687258687258687, '21']\n",
      "[7.948339483394834, '03']\n",
      "[7.94299674267101, '18']\n",
      "[7.713298791018998, '16']\n",
      "[7.5647840531561465, '00']\n",
      "[7.407801418439717, '01']\n",
      "[7.163043478260869, '19']\n",
      "[7.013274336283186, '07']\n",
      "[6.782051282051282, '06']\n",
      "[6.696793002915452, '23']\n",
      "[6.653153153153153, '09']\n"
     ]
    }
   ],
   "source": [
    "sorted_swap = sorted(swap_avg_by_hour, reverse=True)\n",
    "\n",
    "counter = 0\n",
    "for i in sorted_swap:\n",
    "    print(sorted_swap[counter])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can see that hour '15', or 3:00 PM is by far the most popular time to comment on \"Ask HN\" posts. But this output still isn't readable to our average viewer, so let's fix that.\n",
    "\n",
    "## 8. Fixing Output for Readability\n",
    "\n",
    "People tend to enjoy top 5 lists, and we typically print out the first 5 lists of a list-of-lists, so let's keep our new output to the top 5 hours to create \"Ask HN\" posts. We'll need to loop through the averages first, then the hours. We'll parse the hour string, convert it to a readable string format, then print the hour next to the average in a user-friendly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for 'Ask HN' Comments\n",
      "15:00: 28.68 average comments per post\n",
      "13:00: 16.32 average comments per post\n",
      "12:00: 12.38 average comments per post\n",
      "02:00: 11.14 average comments per post\n",
      "10:00: 10.68 average comments per post\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Hours for 'Ask HN' Comments\")\n",
    "\n",
    "for average, hour in sorted_swap[:5]:\n",
    "    \n",
    "    temporary_hour = dt.datetime.strptime(hour, \"%H\") # Parse the hour from the string value.\n",
    "    correct_hour = temporary_hour.strftime(\"%H:%M\")   # Format the hour to HH:MM instead of HH\n",
    "    \n",
    "    print(\"{}: {:.2f} average comments per post\".format(correct_hour,average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial questions were to first find whether \"Ask HN\" or \"Show HN\" posts were more popular. We found that \"Ask HN\" posts were more popular to a significant enough degree to focus our time-analysis on it.\n",
    "\n",
    "The best time create \"Ask HN\" posts would be at 3:00 PM, or 15:00. Therefore, to accrue the most amount of comments on a post, it would need to be an \"Ask HN\" post between 3:00-4:00 PM.\n",
    "\n",
    "It should be noted that some *other* type posts may accrue more comments at a time other than 3:00-4:00 PM. However, these posts are based soley on viral content and are not an unbiased indicator of popularity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
